"""
GitHub Trending çˆ¬è™«
æŠ“å– GitHub æ¯æ—¥çƒ­é—¨é¡¹ç›®
"""
import requests
from bs4 import BeautifulSoup
import time


class GitHubTrendingCrawler:
    def __init__(self):
        self.base_url = 'https://github.com/trending'
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
        }
    
    def get_trending(self, language='', since='daily', max_repos=25):
        """
        è·å– GitHub Trending é¡¹ç›®
        
        Args:
            language: ç¼–ç¨‹è¯­è¨€ï¼Œå¦‚ 'python', 'javascript'
            since: æ—¶é—´èŒƒå›´ 'daily', 'weekly', 'monthly'
            max_repos: æœ€å¤šè¿”å›é¡¹ç›®æ•°
        
        Returns:
            list: é¡¹ç›®åˆ—è¡¨
        """
        try:
            url = f'{self.base_url}/{language}?since={since}'
            print(f"ğŸ“¡ æ­£åœ¨æŠ“å–: {url}")
            
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            repos = []
            articles = soup.find_all('article', class_='Box-row')
            
            for article in articles[:max_repos]:
                try:
                    repo = self._parse_repo(article)
                    if repo:
                        repos.append(repo)
                except Exception as e:
                    print(f"âš ï¸  è§£æé¡¹ç›®å¤±è´¥: {e}")
                    continue
            
            print(f"âœ… æˆåŠŸæŠ“å– {len(repos)} ä¸ª {language or 'All'} é¡¹ç›®")
            return repos
            
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥ [{language}]: {e}")
            return []
    
    def _parse_repo(self, article):
        """è§£æå•ä¸ªä»“åº“ä¿¡æ¯"""
        # è·å–ä»“åº“åç§°å’Œé“¾æ¥
        h2 = article.find('h2', class_='h3')
        if not h2:
            return None
        
        repo_link = h2.find('a')
        if not repo_link:
            return None
        
        repo_name = repo_link.get('href', '').strip('/')
        repo_url = f"https://github.com{repo_link.get('href', '')}"
        
        # è·å–æè¿°
        description_elem = article.find('p', class_='col-9')
        description = description_elem.get_text(strip=True) if description_elem else ''
        
        # è·å–è¯­è¨€
        language_elem = article.find('span', itemprop='programmingLanguage')
        language = language_elem.get_text(strip=True) if language_elem else 'Unknown'
        
        # è·å– stars æ•°é‡
        stars = self._extract_number(article, 'octicon-star')
        
        # è·å– forks æ•°é‡
        forks = self._extract_number(article, 'octicon-repo-forked')
        
        # è·å–ä»Šæ—¥ stars
        stars_today_elem = article.find('span', class_='d-inline-block float-sm-right')
        stars_today = '0'
        if stars_today_elem:
            stars_today_text = stars_today_elem.get_text(strip=True)
            # æå–æ•°å­—ï¼Œä¾‹å¦‚ "123 stars today" -> "123"
            import re
            match = re.search(r'([\d,]+)', stars_today_text)
            if match:
                stars_today = match.group(1)
        
        return {
            'name': repo_name,
            'url': repo_url,
            'description': description,
            'language': language,
            'stars': stars,
            'forks': forks,
            'stars_today': stars_today
        }
    
    def _extract_number(self, article, icon_class):
        """æå–æ•°å­—ä¿¡æ¯ï¼ˆstars, forks ç­‰ï¼‰"""
        svg = article.find('svg', class_=icon_class)
        if svg and svg.parent:
            text = svg.parent.get_text(strip=True)
            # ç§»é™¤é€—å·ï¼Œä¾‹å¦‚ "1,234" -> "1234"
            return text.replace(',', '')
        return '0'
    
    def get_multiple_languages(self, languages, since='daily'):
        """
        è·å–å¤šä¸ªè¯­è¨€çš„ trending
        
        Args:
            languages: è¯­è¨€åˆ—è¡¨
            since: æ—¶é—´èŒƒå›´
        
        Returns:
            dict: {language: [repos]}
        """
        result = {}
        for lang in languages:
            result[lang] = self.get_trending(lang, since)
            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(2)
        
        return result


if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    crawler = GitHubTrendingCrawler()
    
    # æµ‹è¯•å•ä¸ªè¯­è¨€
    python_repos = crawler.get_trending('python', 'daily')
    print(f"\nğŸ Python Trending é¡¹ç›®æ•°: {len(python_repos)}")
    if python_repos:
        print(f"ç¤ºä¾‹é¡¹ç›®: {python_repos[0]['name']}")
    
    # æµ‹è¯•å¤šä¸ªè¯­è¨€
    languages = ['python', 'javascript', 'go']
    all_trending = crawler.get_multiple_languages(languages)
    print(f"\nğŸ“Š æ€»è®¡æŠ“å– {sum(len(repos) for repos in all_trending.values())} ä¸ªé¡¹ç›®")
