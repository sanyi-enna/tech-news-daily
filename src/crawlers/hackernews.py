"""
Hacker News API çˆ¬è™«
æŠ“å– Hacker News çƒ­é—¨æ–°é—»
"""
import requests
from datetime import datetime
import time


class HackerNewsCrawler:
    def __init__(self):
        self.base_url = 'https://hacker-news.firebaseio.com/v0'
        self.session = requests.Session()
    
    def get_top_stories(self, limit=30):
        """
        è·å–çƒ­é—¨æ•…äº‹
        
        Args:
            limit: è¿”å›æ•°é‡
        
        Returns:
            list: æ•…äº‹åˆ—è¡¨
        """
        try:
            print(f"ğŸ“¡ æ­£åœ¨æŠ“å– Hacker News Top Stories...")
            
            # è·å–æ•…äº‹ ID åˆ—è¡¨
            stories_url = f'{self.base_url}/topstories.json'
            response = self.session.get(stories_url, timeout=10)
            response.raise_for_status()
            story_ids = response.json()[:limit]
            
            stories = []
            for i, story_id in enumerate(story_ids):
                try:
                    story = self.get_story(story_id)
                    if story:
                        stories.append(story)
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    if (i + 1) % 10 == 0:
                        print(f"   å·²æŠ“å– {i + 1}/{len(story_ids)}")
                    
                    # é¿å…è¯·æ±‚è¿‡å¿«
                    time.sleep(0.1)
                    
                except Exception as e:
                    print(f"âš ï¸  è·å–æ•…äº‹ {story_id} å¤±è´¥: {e}")
                    continue
            
            print(f"âœ… æˆåŠŸæŠ“å– {len(stories)} æ¡ Hacker News")
            return stories
            
        except Exception as e:
            print(f"âŒ æŠ“å– Hacker News å¤±è´¥: {e}")
            return []
    
    def get_story(self, story_id):
        """
        è·å–å•ä¸ªæ•…äº‹è¯¦æƒ…
        
        Args:
            story_id: æ•…äº‹ ID
        
        Returns:
            dict: æ•…äº‹ä¿¡æ¯
        """
        try:
            story_url = f'{self.base_url}/item/{story_id}.json'
            response = self.session.get(story_url, timeout=10)
            response.raise_for_status()
            story = response.json()
            
            if not story:
                return None
            
            # åªè¿”å› story ç±»å‹çš„å†…å®¹
            if story.get('type') != 'story':
                return None
            
            # æ ¼å¼åŒ–æ—¶é—´
            timestamp = story.get('time', 0)
            published_time = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')
            
            return {
                'id': story.get('id'),
                'title': story.get('title', 'No Title'),
                'url': story.get('url', f'https://news.ycombinator.com/item?id={story_id}'),
                'score': story.get('score', 0),
                'author': story.get('by', 'unknown'),
                'time': published_time,
                'timestamp': timestamp,
                'comments': story.get('descendants', 0),
                'hn_url': f'https://news.ycombinator.com/item?id={story_id}'
            }
            
        except Exception as e:
            print(f"âš ï¸  è·å–æ•…äº‹ {story_id} è¯¦æƒ…å¤±è´¥: {e}")
            return None
    
    def get_best_stories(self, limit=30):
        """è·å–æœ€ä½³æ•…äº‹"""
        try:
            print(f"ğŸ“¡ æ­£åœ¨æŠ“å– Hacker News Best Stories...")
            stories_url = f'{self.base_url}/beststories.json'
            response = self.session.get(stories_url, timeout=10)
            response.raise_for_status()
            story_ids = response.json()[:limit]
            
            stories = []
            for story_id in story_ids:
                story = self.get_story(story_id)
                if story:
                    stories.append(story)
                time.sleep(0.1)
            
            print(f"âœ… æˆåŠŸæŠ“å– {len(stories)} æ¡ Best Stories")
            return stories
            
        except Exception as e:
            print(f"âŒ æŠ“å– Best Stories å¤±è´¥: {e}")
            return []
    
    def get_new_stories(self, limit=30):
        """è·å–æœ€æ–°æ•…äº‹"""
        try:
            print(f"ğŸ“¡ æ­£åœ¨æŠ“å– Hacker News New Stories...")
            stories_url = f'{self.base_url}/newstories.json'
            response = self.session.get(stories_url, timeout=10)
            response.raise_for_status()
            story_ids = response.json()[:limit]
            
            stories = []
            for story_id in story_ids:
                story = self.get_story(story_id)
                if story:
                    stories.append(story)
                time.sleep(0.1)
            
            print(f"âœ… æˆåŠŸæŠ“å– {len(stories)} æ¡ New Stories")
            return stories
            
        except Exception as e:
            print(f"âŒ æŠ“å– New Stories å¤±è´¥: {e}")
            return []


if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    crawler = HackerNewsCrawler()
    
    # æµ‹è¯• Top Stories
    stories = crawler.get_top_stories(10)
    print(f"\nğŸ“° Hacker News Top Stories: {len(stories)} æ¡")
    
    if stories:
        print(f"\nç¤ºä¾‹æ–°é—»:")
        print(f"æ ‡é¢˜: {stories[0]['title']}")
        print(f"è¯„åˆ†: {stories[0]['score']} points")
        print(f"è¯„è®º: {stories[0]['comments']}")
        print(f"ä½œè€…: {stories[0]['author']}")
