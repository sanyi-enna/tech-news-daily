"""
RSS è®¢é˜…æºçˆ¬è™«
æŠ“å–å„å¤§ç§‘æŠ€åª’ä½“çš„ RSS Feed
"""
import feedparser
from datetime import datetime
import time


class RSSFeedCrawler:
    def __init__(self):
        # è‹±æ–‡ç§‘æŠ€åª’ä½“
        self.feeds = {
            'TechCrunch': 'https://techcrunch.com/feed/',
            'The Verge': 'https://www.theverge.com/rss/index.xml',
            'Ars Technica': 'https://feeds.arstechnica.com/arstechnica/index',
            'Wired': 'https://www.wired.com/feed/rss',
            'MIT Technology Review': 'https://www.technologyreview.com/feed/',
        }
        
        # ä¸­æ–‡ç§‘æŠ€åª’ä½“
        self.chinese_feeds = {
            '36æ°ª': 'https://36kr.com/feed',
            'å°‘æ•°æ´¾': 'https://sspai.com/feed',
            'InfoQ': 'https://www.infoq.cn/feed',
        }
    
    def parse_feed(self, url, source_name, limit=10):
        """
        è§£æå•ä¸ª RSS Feed
        
        Args:
            url: RSS Feed URL
            source_name: æ¥æºåç§°
            limit: è¿”å›æ•°é‡
        
        Returns:
            list: æ–‡ç« åˆ—è¡¨
        """
        try:
            print(f"ğŸ“¡ æ­£åœ¨æŠ“å– {source_name}...")
            
            feed = feedparser.parse(url)
            
            if feed.bozo:  # è§£æé”™è¯¯
                print(f"âš ï¸  RSS è§£æè­¦å‘Š: {source_name}")
            
            articles = []
            for entry in feed.entries[:limit]:
                try:
                    article = {
                        'title': entry.get('title', 'No Title'),
                        'url': entry.get('link', ''),
                        'source': source_name,
                        'published': self._format_time(entry),
                        'summary': self._get_summary(entry),
                        'author': entry.get('author', 'Unknown'),
                    }
                    articles.append(article)
                except Exception as e:
                    print(f"âš ï¸  è§£ææ¡ç›®å¤±è´¥: {e}")
                    continue
            
            print(f"âœ… {source_name}: {len(articles)} ç¯‡æ–‡ç« ")
            return articles
            
        except Exception as e:
            print(f"âŒ æŠ“å– {source_name} å¤±è´¥: {e}")
            return []
    
    def _format_time(self, entry):
        """æ ¼å¼åŒ–å‘å¸ƒæ—¶é—´"""
        if hasattr(entry, 'published_parsed') and entry.published_parsed:
            try:
                dt = datetime(*entry.published_parsed[:6])
                return dt.strftime('%Y-%m-%d %H:%M:%S')
            except:
                pass
        
        if hasattr(entry, 'published'):
            return entry.published
        
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    def _get_summary(self, entry):
        """è·å–æ‘˜è¦"""
        # å°è¯•å¤šä¸ªå¯èƒ½çš„æ‘˜è¦å­—æ®µ
        if hasattr(entry, 'summary'):
            # ç§»é™¤ HTML æ ‡ç­¾
            summary = entry.summary
            from html.parser import HTMLParser
            
            class MLStripper(HTMLParser):
                def __init__(self):
                    super().__init__()
                    self.reset()
                    self.fed = []
                def handle_data(self, d):
                    self.fed.append(d)
                def get_data(self):
                    return ''.join(self.fed)
            
            s = MLStripper()
            s.feed(summary)
            clean_summary = s.get_data()
            
            # é™åˆ¶é•¿åº¦
            return clean_summary[:200] + '...' if len(clean_summary) > 200 else clean_summary
        
        if hasattr(entry, 'description'):
            return entry.description[:200] + '...' if len(entry.description) > 200 else entry.description
        
        return ''
    
    def get_all_feeds(self, limit_per_feed=10, include_chinese=True):
        """
        è·å–æ‰€æœ‰è®¢é˜…æº
        
        Args:
            limit_per_feed: æ¯ä¸ªæºçš„æ–‡ç« æ•°é‡
            include_chinese: æ˜¯å¦åŒ…å«ä¸­æ–‡æº
        
        Returns:
            list: æ‰€æœ‰æ–‡ç« åˆ—è¡¨
        """
        all_articles = []
        
        # æŠ“å–è‹±æ–‡æº
        for source, url in self.feeds.items():
            articles = self.parse_feed(url, source, limit_per_feed)
            all_articles.extend(articles)
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        # æŠ“å–ä¸­æ–‡æº
        if include_chinese:
            for source, url in self.chinese_feeds.items():
                articles = self.parse_feed(url, source, limit_per_feed)
                all_articles.extend(articles)
                time.sleep(1)
        
        print(f"\nâœ… æ€»è®¡æŠ“å– {len(all_articles)} ç¯‡æ–‡ç« ")
        return all_articles
    
    def get_feeds_by_category(self, category='english', limit=10):
        """
        æŒ‰ç±»åˆ«è·å–è®¢é˜…æº
        
        Args:
            category: 'english' æˆ– 'chinese'
            limit: æ¯ä¸ªæºçš„æ•°é‡
        
        Returns:
            dict: {source: [articles]}
        """
        result = {}
        feeds = self.feeds if category == 'english' else self.chinese_feeds
        
        for source, url in feeds.items():
            result[source] = self.parse_feed(url, source, limit)
            time.sleep(1)
        
        return result


if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    crawler = RSSFeedCrawler()
    
    # æµ‹è¯•å•ä¸ªæº
    articles = crawler.parse_feed(
        'https://techcrunch.com/feed/',
        'TechCrunch',
        5
    )
    print(f"\nğŸ“° TechCrunch æ–‡ç« : {len(articles)} ç¯‡")
    if articles:
        print(f"ç¤ºä¾‹æ ‡é¢˜: {articles[0]['title']}")
    
    # æµ‹è¯•æ‰€æœ‰æº
    all_articles = crawler.get_all_feeds(limit_per_feed=5, include_chinese=False)
    print(f"\nğŸ“Š æ€»è®¡: {len(all_articles)} ç¯‡æ–‡ç« ")
