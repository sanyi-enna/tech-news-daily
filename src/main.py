"""
ä¸»ç¨‹åº - æ¯æ—¥ç§‘æŠ€èµ„è®¯èšåˆå™¨
Tech News Daily Aggregator
"""
import json
import os
from datetime import datetime
from crawlers.github_trending import GitHubTrendingCrawler
from crawlers.hackernews import HackerNewsCrawler
from crawlers.rss_feeds import RSSFeedCrawler


class TechNewsAggregator:
    def __init__(self):
        self.github_crawler = GitHubTrendingCrawler()
        self.hn_crawler = HackerNewsCrawler()
        self.rss_crawler = RSSFeedCrawler()
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs('data/archive', exist_ok=True)
        os.makedirs('docs/data', exist_ok=True)
    
    def crawl_all(self):
        """æ‰§è¡Œæ‰€æœ‰çˆ¬è™«ä»»åŠ¡"""
        print("="*60)
        print("ğŸš€ å¼€å§‹æŠ“å–ç§‘æŠ€èµ„è®¯...")
        print("="*60)
        
        timestamp = datetime.now().isoformat()
        date_str = datetime.now().strftime('%Y-%m-%d')
        
        # 1. æŠ“å– GitHub Trending
        print("\nğŸ“Š [1/3] GitHub Trending")
        print("-"*60)
        github_languages = ['python', 'javascript', 'go', 'rust', 'java', 'typescript']
        github_trending = self.github_crawler.get_multiple_languages(github_languages, 'daily')
        
        # 2. æŠ“å– Hacker News
        print("\nğŸ“° [2/3] Hacker News")
        print("-"*60)
        hackernews = self.hn_crawler.get_top_stories(limit=30)
        
        # 3. æŠ“å– RSS è®¢é˜…æº
        print("\nğŸ“¡ [3/3] RSS Feeds")
        print("-"*60)
        rss_articles = self.rss_crawler.get_all_feeds(limit_per_feed=10, include_chinese=True)
        
        # ä¿å­˜æ•°æ®
        print("\nğŸ’¾ ä¿å­˜æ•°æ®...")
        self.save_data(timestamp, date_str, github_trending, hackernews, rss_articles)
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        self.print_statistics(github_trending, hackernews, rss_articles)
        
        print("\n" + "="*60)
        print("âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
        print("="*60)
    
    def save_data(self, timestamp, date_str, github_trending, hackernews, rss_articles):
        """ä¿å­˜æŠ“å–çš„æ•°æ®"""
        
        # 1. ä¿å­˜ GitHub Trending
        github_data = {
            'timestamp': timestamp,
            'date': date_str,
            'data': github_trending
        }
        self._save_json('data/github_trending.json', github_data)
        
        # 2. ä¿å­˜ Hacker News
        hn_data = {
            'timestamp': timestamp,
            'date': date_str,
            'data': hackernews
        }
        self._save_json('data/hackernews.json', hn_data)
        
        # 3. ä¿å­˜ RSS æ–‡ç« 
        rss_data = {
            'timestamp': timestamp,
            'date': date_str,
            'data': rss_articles
        }
        self._save_json('data/rss_feeds.json', rss_data)
        
        # 4. ç”Ÿæˆåˆå¹¶çš„æœ€æ–°æ•°æ®ï¼ˆä¾›å‰ç«¯ä½¿ç”¨ï¼‰
        latest_data = {
            'updated_at': timestamp,
            'date': date_str,
            'github_trending': github_trending,
            'hackernews': hackernews[:20],  # åªä¿ç•™å‰20æ¡
            'rss_feeds': rss_articles[:30],  # åªä¿ç•™å‰30æ¡
            'statistics': {
                'github_repos': sum(len(repos) for repos in github_trending.values()),
                'hackernews_stories': len(hackernews),
                'rss_articles': len(rss_articles),
                'total': sum(len(repos) for repos in github_trending.values()) + len(hackernews) + len(rss_articles)
            }
        }
        
        # ä¿å­˜åˆ° data ç›®å½•
        self._save_json('data/latest.json', latest_data)
        
        # åŒæ­¥åˆ° docs ç›®å½•ï¼ˆä¾› GitHub Pages ä½¿ç”¨ï¼‰
        self._save_json('docs/data/latest.json', latest_data)
        
        # 5. ä¿å­˜æ¯æ—¥å½’æ¡£
        archive_file = f'data/archive/{date_str}.json'
        archive_data = {
            'date': date_str,
            'timestamp': timestamp,
            'github_trending': github_trending,
            'hackernews': hackernews,
            'rss_feeds': rss_articles
        }
        self._save_json(archive_file, archive_data)
        
        print(f"   âœ“ æ•°æ®å·²ä¿å­˜")
        print(f"   âœ“ å½’æ¡£: {archive_file}")
    
    def _save_json(self, filepath, data):
        """ä¿å­˜ JSON æ–‡ä»¶"""
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥ {filepath}: {e}")
    
    def print_statistics(self, github_trending, hackernews, rss_articles):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯")
        print("-"*60)
        
        github_total = sum(len(repos) for repos in github_trending.values())
        print(f"   GitHub Trending: {github_total} ä¸ªé¡¹ç›®")
        for lang, repos in github_trending.items():
            print(f"      - {lang}: {len(repos)} ä¸ª")
        
        print(f"   Hacker News: {len(hackernews)} æ¡æ–°é—»")
        print(f"   RSS Feeds: {len(rss_articles)} ç¯‡æ–‡ç« ")
        
        # RSS æŒ‰æ¥æºç»Ÿè®¡
        rss_by_source = {}
        for article in rss_articles:
            source = article.get('source', 'Unknown')
            rss_by_source[source] = rss_by_source.get(source, 0) + 1
        
        for source, count in sorted(rss_by_source.items()):
            print(f"      - {source}: {count} ç¯‡")
        
        total = github_total + len(hackernews) + len(rss_articles)
        print(f"   æ€»è®¡: {total} æ¡èµ„è®¯")


def main():
    """ä¸»å‡½æ•°"""
    aggregator = TechNewsAggregator()
    aggregator.crawl_all()


if __name__ == '__main__':
    main()
